{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.cross_validation as skv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.tree as skt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Week 9 - Tree Based Methods </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate Data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data <font face =\"Courier\">(X,y)</font> so that the relationship looks like the figure below. Both should be (25,1) arrays. Note that there's no error term!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 5, 25).reshape(25, 1)\n",
    "y = 5/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b> \n",
    "\n",
    "+ If you could fit *one horizontal line* to this data, at what height would you fit it? [Based on what criterion?]\n",
    "\n",
    "+ If you could fit *two horizontal lines*, where would you place them?\n",
    "\n",
    "+ How about more horizontal lines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use <font face=\"Courier\">sklearn.tree.DecisionTreeRegressor</font> to fit trees of different <font face=\"Courier\">max_depth</font>s. Then plot the predicted value over a grid of $X$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = skt.DecisionTreeRegressor(max_depth = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "tree.fit(X, y)\n",
    "# Predict over x_grid\n",
    "x_grid = np.linspace(-5, 5, 1000)[:,np.newaxis]\n",
    "yhat = tree.predict(x_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (15,7))\n",
    "ax.grid()\n",
    "plt.scatter(x, y, s = 200)\n",
    "plt.plot(x_grid, yhat, linewidth = 5, color = \"darkorange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Discussion</b> \n",
    "\n",
    "+ Which variables are we optimizing over?\n",
    "\n",
    "+ How many leaves are there when max_depth = 2? 4? 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Bias vs. Variance in Trees</b>\n",
    "\n",
    "Let's introduce some error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = 1\n",
    "n_bootstrap = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (12,6))\n",
    "\n",
    "n_obs = 25\n",
    "XX = np.linspace(-5, 5, n_obs).reshape(n_obs, 1)\n",
    "yy = y + np.random.normal(0, .5, size = (n_obs, 1))\n",
    "\n",
    "plt.scatter(XX, yy, s = 250, alpha = .5)\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    \n",
    "    idx = np.random.choice(range(n_obs), replace = True, size = n_obs)\n",
    "    tree = skt.DecisionTreeRegressor(max_depth = max_depth) # Note the max_depth!\n",
    "    tree.fit(XX[idx], yy[idx])\n",
    "    yhat = tree.predict(x_grid)\n",
    "    plt.plot(x_grid, yhat, linewidth = 2, color = \"darkorange\")\n",
    "\n",
    "    \n",
    "x_grid = np.linspace(-5, 5, 1000)[:,np.newaxis]\n",
    "y_true = 5/(1 + np.exp(-x_grid))\n",
    "ax.plot(x_grid, y_true, linewidth = 5)\n",
    "\n",
    "#ax.set_title(\"Bootstrap iterations: %d\" % n_bootstrap, fontsize = 20)\n",
    "ax.text(-4, 4, \"Max Depth: %d\" % tree.max_depth, fontsize = 20)\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Discussion</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ How does the variance of *one* tree behave as we increase the number of leaves? \n",
    "\n",
    "+ When we resample the data, how does the *average* perform ? \n",
    "\n",
    "+ Draw an insight to the previous question to propose an estimator. Then, immediately time-travel to 25 years ago, publish your idea in an academic paper and go enjoy your professorship in Stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>By the way</b>\n",
    "\n",
    "+ Don't worry about algorithm 8.1 in the book unless you're really into trees. \n",
    "\n",
    "+ No one uses trees anymore. They've been superseeded by...\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 1>[This is one of my favorite algorithms.]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warm-up</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Given a bunch of data $\\{Z_{i}\\}$ *iid* with mean $\\mu$ and variance $\\sigma^2$, propose an estimator for $E[Z]$.\n",
    "\n",
    "+ What is the variance of that estimator?\n",
    "\n",
    "+ What if your data is not independently drawn? [Hint: think about extreme cases of non-independence.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Intuition</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *random* in *random* forests comes from the idea of \"decorrelating\" each tree estimate, by injecting randomness in the estimate. This is accomplished by:\n",
    "\n",
    "+ Bagging\n",
    "\n",
    "+ Not using all regressors to build each tree (!)\n",
    "\n",
    "Sounds stupid, works wonders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the *Heart* dataset, as in the textbook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv(\"heart.csv\", index_col = 0).dropna()\n",
    "heart[\"ChestPain\"] = heart[\"ChestPain\"].replace({\"asymptomatic\":0, \"typical\":1,  \"nonanginal\":2, \"nontypical\":3})\n",
    "heart[\"AHD\"] = heart[\"AHD\"].replace({\"Yes\":1, \"No\":0})\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = heart[\"AHD\"]\n",
    "X = heart[[\"Age\", \"Sex\", \"ChestPain\", \"RestBP\", \"Chol\", \"Fbs\", \"RestECG\", \"MaxHR\", \"ExAng\", \"Oldpeak\", \"Ca\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use <font face = \"Courier\">sklearn.ensemble.RandomForestClassifier</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = ske.RandomForestClassifier(n_estimators = 1, oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.fit(X, y)\n",
    "print(rf.oob_score_)  # If n_estimators is low, it will produce a warning. Don't worry about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Variable Importance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (10, 5))\n",
    "pd.Series(index = X.columns, data = rf.feature_importances_).order().plot(ax = ax, kind = \"barh\", color = \"purple\")\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14) \n",
    "ax.set_title(\"Variable Importance\", fontsize = 20)\n",
    "ax.set_xlabel(\"Reduction in accuracy when variable is randomized\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>\n",
    "\n",
    "Feel free to use this in your HW5 / Final Project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
